{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Theoretical Questions**\n",
        "\n",
        "**1. Can we use Bagging for regression problems?**\n",
        "\n",
        "Answer: Yes, Bagging (Bootstrap Aggregating) can be used for regression problems. Bagging is a general ensemble technique that reduces variance by training multiple models on different subsets of the data (created via bootstrap sampling) and combining their predictions. For regression, the final prediction is typically the average of the predictions from all models, which helps stabilize the output and reduce overfitting. For example, a Bagging Regressor can use decision trees as base learners to predict continuous values.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2. What is the difference between multiple model training and single model training?**\n",
        "\n",
        "\n",
        "Answer:\n",
        "Single Model Training: Involves training one model on the entire dataset. The performance depends heavily on the chosen algorithm, hyperparameters, and the quality of the data. Single models are often simpler, faster to train, but can suffer from high variance (overfitting) or high bias (underfitting).\n",
        "Multiple Model Training: Involves training several models, often on different subsets of data or with different algorithms, and combining their predictions (e.g., via averaging or voting). This is the foundation of ensemble methods like Bagging, Boosting, and Stacking. Multiple model training reduces variance, improves generalization, and is more robust but is computationally more expensive and complex to implement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**3. Explain the concept of feature randomness in Random Forest.**\n",
        "\n",
        "\n",
        "Answer: Feature randomness is a key concept in Random Forest that enhances the diversity of individual trees and reduces correlation between them. In a Random Forest, at each split in a decision tree, instead of considering all features to find the best split, only a random subset of features is considered. This subset is controlled by the max_features hyperparameter. For example, if max_features=sqrt(n_features), only a square root of the total features is considered at each split. This randomness ensures that trees are less correlated, making the ensemble more robust and reducing overfitting compared to a single decision tree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "Answer: The Out-of-Bag (OOB) Score is an internal validation metric used in Bagging and Random Forest models. In Bagging, each base model (e.g., decision tree) is trained on a bootstrap sample of the data, meaning some data points are left out (not included in the training subset). These left-out samples are called out-of-bag samples. The OOB Score is calculated by using these OOB samples to evaluate the model's performance without needing a separate validation set. For classification, it is typically the accuracy on OOB samples, and for regression, it is the mean squared error (MSE) or RÂ² score. The OOB Score provides a convenient way to estimate model performance without cross-validation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**5. How can you measure the importance of features in a Random Forest model ?**\n",
        "Answer: Feature importance in a Random Forest model is typically measured using one of the following methods:\n",
        "\n",
        "Mean Decrease in Impurity (MDI): This is the default method in libraries like scikit-learn. It measures the total reduction in impurity (e.g., Gini impurity for classification or variance for regression) brought by a feature across all trees. Features that lead to larger impurity reductions are considered more important.\n",
        "\n",
        "Mean Decrease in Accuracy (MDA) or Permutation Importance: This method involves shuffling the values of a feature and measuring the decrease in model accuracy on OOB samples or a validation set. A larger decrease indicates higher importance. This method is more robust but computationally expensive.\n",
        "Feature importance scores are normalized to sum to 1, making it easy to interpret their relative importance.\n",
        "\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier.**\n",
        "\n",
        "\n",
        "Answer: A Bagging Classifier works on the principle of Bootstrap Aggregating to reduce variance and improve model stability. The steps are:\n",
        "\n",
        "Bootstrap Sampling: Generate multiple subsets of the training data by sampling with replacement (bootstrap sampling). Each subset is of the same size as the original dataset but may contain duplicates and miss some original data points.\n",
        "Model Training: Train a base classifier (e.g., decision tree) independently on each bootstrap subset. Each classifier learns a slightly different perspective of the data due to the variation in the subsets.\n",
        "\n",
        "Prediction Aggregation: For a new data point, obtain predictions from all base classifiers and aggregate them. For classification, this is typically done by majority voting, where the class with the most votes is chosen as the final prediction.\n",
        "\n",
        "Variance Reduction: By averaging out predictions, Bagging reduces variance, making the model less sensitive to noise in the data compared to a single classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**7. How do you evaluate a Bagging Classifier's performance?**\n",
        "\n",
        "Answer: The performance of a Bagging Classifier can be evaluated using standard classification metrics, depending on the problem context. Common evaluation methods include:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances (suitable for balanced datasets).\n",
        "\n",
        "Precision, Recall, and F1-Score: Useful for imbalanced datasets, where precision measures the accuracy of positive predictions, recall measures the coverage of actual positives, and F1-score is the harmonic mean of precision and recall.\n",
        "\n",
        "ROC-AUC Score: Measures the area under the Receiver Operating Characteristic curve, indicating the model's ability to distinguish between classes.\n",
        "Confusion Matrix: Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Cross-Validation: Use k-fold cross-validation to assess the model's performance on different splits of the data, ensuring robustness.\n",
        "\n",
        "Out-of-Bag (OOB) Score: If available, use the OOB score as an internal validation metric without needing a separate test set.\n",
        "\n",
        "\n",
        "**8. How does a Bagging Regressor work?**\n",
        "\n",
        "Answer: A Bagging Regressor works similarly to a Bagging Classifier but is designed for regression tasks. The steps are:\n",
        "Bootstrap Sampling: Generate multiple subsets of the training data by sampling with replacement.\n",
        "\n",
        "Model Training: Train a base regressor (e.g., decision tree) independently on each bootstrap subset.\n",
        "\n",
        "Prediction Aggregation: For a new data point, obtain predictions from all base regressors and aggregate them, typically by taking the average (mean) of the predictions.\n",
        "\n",
        "Variance Reduction: By averaging predictions, the Bagging Regressor reduces variance and stabilizes predictions, making it less prone to overfitting compared to a single regressor. This is particularly useful for high-variance models like decision trees.\n",
        "\n",
        "\n",
        "**9. What is the main advantage of ensemble techniques?**\n",
        "\n",
        "Answer: The main advantage of ensemble techniques is improved predictive performance through reduced variance, bias, or both, leading to better generalization on unseen data. By combining multiple models, ensemble methods leverage the strengths of individual models while mitigating their weaknesses. For example:\n",
        "\n",
        "Bagging reduces variance by averaging predictions, making models more stable.\n",
        "Boosting reduces bias by focusing on hard-to-predict examples, improving accuracy.\n",
        "\n",
        "Stacking combines diverse models to capture different patterns, enhancing overall performance.\n",
        "\n",
        "**10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "Answer: The main challenge of ensemble methods is increased computational complexity. Ensemble methods require training and maintaining multiple models, which can be computationally expensive and resource-intensive, especially for large datasets or complex models. Other challenges include:\n",
        "\n",
        "Interpretability: Ensemble models (e.g., Random Forest, Boosting) are harder to interpret compared to single models like linear regression or decision trees.\n",
        "Overfitting Risk: If not properly tuned, ensemble methods can still overfit, especially Boosting methods that focus on hard examples.\n",
        "\n",
        "Hyperparameter Tuning: Ensemble methods often have many hyperparameters (e.g., number of estimators, learning rate in Boosting) that require careful tuning.\n",
        "\n",
        "**11. Explain the key idea behind ensemble techniques.**\n",
        "\n",
        "Answer: The key idea behind ensemble techniques is to combine the predictions of multiple models to achieve better performance than any single model could achieve alone. This is based on the principle of \"wisdom of the crowd,\" where diverse models, each with its own strengths and weaknesses, can complement each other. The main approaches are:\n",
        "\n",
        "Bagging: Reduces variance by training models on different data subsets and averaging predictions.\n",
        "\n",
        "Boosting: Reduces bias by sequentially training models, where each model corrects the errors of the previous ones.\n",
        "\n",
        "Stacking: Combines different types of models using a meta-model to learn how to best combine their predictions.\n",
        "\n",
        "**12. What is a Random Forest Classifier?**\n",
        "\n",
        "Answer: A Random Forest Classifier is an ensemble learning method that combines multiple decision trees to improve classification performance. It operates on the principles of Bagging and feature randomness. The key features are:\n",
        "Bootstrap Sampling: Each tree is trained on a different bootstrap sample of the data.\n",
        "Feature Randomness: At each split in a tree, only a random subset of features is considered, reducing correlation between trees.\n",
        "Prediction Aggregation: For classification, the final prediction is made by majority voting across all trees.\n",
        "Random Forest is robust, resistant to overfitting (compared to a single decision tree), and can handle high-dimensional data well.\n",
        "\n",
        "**13. What are the main types of ensemble techniques?**\n",
        "\n",
        "Answer: The main types of ensemble techniques are:\n",
        "Bagging (Bootstrap Aggregating): Trains multiple models in parallel on different bootstrap samples of the data and aggregates predictions (e.g., Random Forest).\n",
        "Boosting: Trains models sequentially, where each model focuses on correcting the errors of the previous models (e.g., AdaBoost, Gradient Boosting, XGBoost).\n",
        "Stacking (Stacked Generalization): Trains multiple diverse models (base learners) and uses a meta-model to combine their predictions, learning how to best weigh each model's contribution.\n",
        "Voting: Combines predictions from multiple models, either by majority voting (classification) or averaging (regression), without necessarily using bootstrap sampling or sequential training.\n",
        "\n",
        "**14. What is ensemble learning in machine learning?**\n",
        "\n",
        "Answer: Ensemble learning in machine learning is a technique that combines the predictions of multiple models (base learners) to improve overall performance, robustness, and generalization compared to a single model. The goal is to leverage the strengths of individual models while mitigating their weaknesses. Ensemble learning is widely used in tasks like classification, regression, and anomaly detection. Examples include Random Forest, Gradient Boosting, and Stacking. Ensemble methods are particularly effective when base models are diverse and complementary.\n",
        "\n",
        "**15. When should we avoid using ensemble methods?**\n",
        "\n",
        "Answer: Ensemble methods should be avoided in the following scenarios:\n",
        "Limited Computational Resources: Ensemble methods are computationally expensive, so they may not be feasible on resource-constrained systems or when fast inference is required.\n",
        "Small or Simple Datasets: For small datasets or simple problems where a single model (e.g., linear regression) performs well, ensemble methods may add unnecessary complexity without significant performance gains.\n",
        "Interpretability Requirements: If interpretability is critical (e.g., in medical or legal applications), ensemble methods are harder to interpret compared to single models like decision trees or logistic regression.\n",
        "Risk of Overfitting: If the dataset is noisy or the ensemble is not properly regularized, ensemble methods (especially Boosting) can overfit.\n",
        "\n",
        "\n",
        "\n",
        "**16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "Answer: Bagging helps in reducing overfitting by reducing the variance of the model. The mechanism is as follows:\n",
        "Variance Reduction: Individual models (e.g., decision trees) trained on the full dataset can have high variance, meaning they are sensitive to small changes in the training data and may overfit. Bagging trains multiple models on different bootstrap samples, which introduces diversity. Aggregating predictions (e.g., via averaging or voting) smooths out these variations, leading to a more stable and generalizable model.\n",
        "Robustness to Noise: By training on different subsets, Bagging ensures that noise or outliers in one subset do not overly influence the entire model, as other models trained on different subsets can compensate.\n",
        "This is particularly effective for high-variance models like decision trees, where a single tree can easily overfit.\n",
        "\n",
        "\n",
        "**17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "Answer: Random Forest is better than a single Decision Tree due to the following reasons:\n",
        "Reduced Overfitting: A single decision tree can overfit by growing too deep and capturing noise in the data. Random Forest reduces overfitting by averaging predictions from multiple trees, each trained on different bootstrap samples and feature subsets.\n",
        "Improved Generalization: The diversity introduced by bootstrap sampling and feature randomness ensures that Random Forest generalizes better to unseen data compared to a single tree.\n",
        "Robustness: Random Forest is less sensitive to noise and outliers because errors in individual trees are averaged out.\n",
        "Feature Importance: Random Forest provides feature importance scores, which are useful for understanding the data, while a single tree does not offer this directly.\n",
        "Performance: Random Forest typically achieves higher accuracy, especially on complex datasets, due to the ensemble effect.\n",
        "\n",
        "**18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "Answer: Bootstrap sampling is a fundamental component of Bagging and plays the following roles:\n",
        "Diversity Creation: Bootstrap sampling creates multiple subsets of the training data by sampling with replacement. Each subset is different, ensuring that the base models (e.g., decision trees) are trained on diverse data, which reduces correlation between models.\n",
        "Variance Reduction: By training models on different subsets and aggregating predictions, bootstrap sampling helps reduce the variance of the final model, making it more stable and less prone to overfitting.\n",
        "OOB Evaluation: Bootstrap sampling leaves out some data points in each subset (out-of-bag samples), which can be used to evaluate the model's performance without needing a separate validation set, providing an efficient internal validation mechanism.\n",
        "\n",
        "\n",
        "**19. What are some real-world applications of ensemble techniques?**\n",
        "Answer: Ensemble techniques are widely used in various real-world applications due to their robustness and high performance. Examples include:\n",
        "Finance: Credit scoring, fraud detection, and stock price prediction, where ensemble methods like Random Forest or Gradient Boosting handle high-dimensional and noisy data effectively.\n",
        "Healthcare: Disease diagnosis (e.g., cancer detection using Random Forest on medical imaging data), patient outcome prediction, and drug discovery, where ensemble methods improve accuracy and robustness.\n",
        "Marketing: Customer segmentation, churn prediction, and recommendation systems, where ensemble methods handle complex patterns in customer behavior data.\n",
        "Natural Language Processing (NLP): Sentiment analysis, text classification, and machine translation, where stacking or boosting models improve performance.\n",
        "Computer Vision: Object detection, image classification, and facial recognition, where ensemble methods combine features from different models for better accuracy.\n",
        "Kaggle Competitions: Ensemble methods (e.g., XGBoost, LightGBM) are often used to win machine learning competitions due to their superior performance on structured data.\n",
        "\n",
        "**20. What is the difference between Bagging and Boosting?**\n",
        "Answer:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "Training Approach: Models are trained independently in parallel on different bootstrap samples of the data.\n",
        "\n",
        "Goal: Reduces variance, making the model more stable and less prone to overfitting.\n",
        "\n",
        "Aggregation: Predictions are aggregated by averaging (regression) or majority voting (classification).\n",
        "\n",
        "Example: Random Forest.\n",
        "Sensitivity to Errors: Less sensitive to noisy data, as each model is independent.\n",
        "\n",
        "Boosting:\n",
        "Training Approach: Models are trained sequentially, where each model focuses on correcting the errors of the previous models by assigning higher weights to misclassified or poorly predicted instances.\n",
        "\n",
        "Goal: Reduces bias, improving the accuracy of weak learners to create a strong learner.\n",
        "\n",
        "Aggregation: Predictions are combined using a weighted sum or weighted voting, where later models often have higher influence.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
        "Sensitivity to Errors: More sensitive to noisy data, as errors in early models can propagate and affect later models."
      ],
      "metadata": {
        "id": "qU_gMgfa66yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Questions**\n",
        "\n",
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "Approach: Use BaggingClassifier from sklearn.ensemble with DecisionTreeClassifier as the base estimator. Evaluate on a test set using accuracy."
      ],
      "metadata": {
        "id": "Io5dajEK-QrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSSoRKuu-YqM",
        "outputId": "8d7b102a-21a6-46e0-b585-136e2169339a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "Approach: Use BaggingRegressor with DecisionTreeRegressor as the base estimator. Evaluate on a test set using MSE.\n",
        "Sample Code:"
      ],
      "metadata": {
        "id": "qi112PRP-hbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPlyi5fc-q6i",
        "outputId": "8a5dff2e-31db-4c0f-e866-ee940ce774d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 3222.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "Approach: Use RandomForestClassifier and access feature importance scores via the feature_importances_ attribute."
      ],
      "metadata": {
        "id": "wEzb64NU_NXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_names = data.feature_names\n",
        "importances = rf.feature_importances_\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"Feature: {name}, Importance: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4StTjUD_OCN",
        "outputId": "67c96710-8f1b-4449-d1f1-dfca2c8d75ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: mean radius, Importance: 0.0487\n",
            "Feature: mean texture, Importance: 0.0136\n",
            "Feature: mean perimeter, Importance: 0.0533\n",
            "Feature: mean area, Importance: 0.0476\n",
            "Feature: mean smoothness, Importance: 0.0073\n",
            "Feature: mean compactness, Importance: 0.0139\n",
            "Feature: mean concavity, Importance: 0.0680\n",
            "Feature: mean concave points, Importance: 0.1062\n",
            "Feature: mean symmetry, Importance: 0.0038\n",
            "Feature: mean fractal dimension, Importance: 0.0039\n",
            "Feature: radius error, Importance: 0.0201\n",
            "Feature: texture error, Importance: 0.0047\n",
            "Feature: perimeter error, Importance: 0.0113\n",
            "Feature: area error, Importance: 0.0224\n",
            "Feature: smoothness error, Importance: 0.0043\n",
            "Feature: compactness error, Importance: 0.0053\n",
            "Feature: concavity error, Importance: 0.0094\n",
            "Feature: concave points error, Importance: 0.0035\n",
            "Feature: symmetry error, Importance: 0.0040\n",
            "Feature: fractal dimension error, Importance: 0.0053\n",
            "Feature: worst radius, Importance: 0.0780\n",
            "Feature: worst texture, Importance: 0.0217\n",
            "Feature: worst perimeter, Importance: 0.0671\n",
            "Feature: worst area, Importance: 0.1539\n",
            "Feature: worst smoothness, Importance: 0.0106\n",
            "Feature: worst compactness, Importance: 0.0203\n",
            "Feature: worst concavity, Importance: 0.0318\n",
            "Feature: worst concave points, Importance: 0.1447\n",
            "Feature: worst symmetry, Importance: 0.0101\n",
            "Feature: worst fractal dimension, Importance: 0.0052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "Approach: Train both RandomForestRegressor and DecisionTreeRegressor on the same dataset and compare MSE."
      ],
      "metadata": {
        "id": "Pc0hJo_S_SiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Random Forest MSE: {rf_mse:.2f}\")\n",
        "print(f\"Decision Tree MSE: {dt_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRngw1cK_S55",
        "outputId": "909aada1-17bf-4980-aa52-d3ffd26f41b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE: 2621.79\n",
            "Decision Tree MSE: 6350.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "Approach: Use RandomForestClassifier with oob_score=True to compute the OOB score."
      ],
      "metadata": {
        "id": "dkWTEkli_Zzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest Classifier with OOB score\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"Out-of-Bag Score: {rf.oob_score_:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj5KAPZ4_aHE",
        "outputId": "30b0d19c-0683-4f08-ef3e-c9946a6413cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag Score: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "Approach: Use BaggingClassifier with SVC as the base estimator."
      ],
      "metadata": {
        "id": "crK0ASRd_g8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with SVM\n",
        "bagging = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mLQssNT_hYr",
        "outputId": "370119dd-bb30-46b2-b781-0b449b11a95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "Approach: Use BaggingClassifier with LogisticRegression and evaluate using ROC-AUC score."
      ],
      "metadata": {
        "id": "hn62tzrm_oVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression\n",
        "bagging = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate AUC\n",
        "y_pred_proba = bagging.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKhd1FeO_o11",
        "outputId": "2ea6502d-56f8-4e35-f384-ece4e7da2b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "Approach: Train BaggingClassifier and RandomForestClassifier and compare accuracy."
      ],
      "metadata": {
        "id": "Wgf2RYlT_vLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.2f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDAEPFmW_vd6",
        "outputId": "6489587d-0780-475c-a68c-8413ad84e5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.00\n",
            "Random Forest Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "Approach: Train BaggingRegressor with varying n_estimators and evaluate MSE."
      ],
      "metadata": {
        "id": "Rq_EsWGD_0U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with different numbers of estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "for n in n_estimators_list:\n",
        "    bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Number of Estimators: {n}, MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKv6sPLo_0n7",
        "outputId": "3f04ca4f-30cb-4898-c45c-03d03bd2ee20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Estimators: 10, MSE: 3222.40\n",
            "Number of Estimators: 50, MSE: 2664.08\n",
            "Number of Estimators: 100, MSE: 2631.67\n",
            "Number of Estimators: 200, MSE: 2593.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples.\n",
        "Approach: Train RandomForestClassifier, identify misclassified samples, and analyze their characteristics."
      ],
      "metadata": {
        "id": "ers_I-n3_62C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and identify misclassified samples\n",
        "y_pred = rf.predict(X_test)\n",
        "misclassified_idx = np.where(y_test != y_pred)[0]\n",
        "\n",
        "# Analyze misclassified samples\n",
        "print(\"Misclassified Samples:\")\n",
        "for idx in misclassified_idx:\n",
        "    print(f\"Sample {idx}: True Label = {y_test[idx]}, Predicted Label = {y_pred[idx]}, Features = {X_test[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT5R8X_5_7XT",
        "outputId": "aeef6e21-cc87-4641-cb63-ee782a46cf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Samples:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "Approach: Train BaggingClassifier and DecisionTreeClassifier and compare accuracy."
      ],
      "metadata": {
        "id": "GNa1d_ck__Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.2f}\")\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKLw9nL4__lQ",
        "outputId": "17273d03-6e7c-4951-8d33-e7be702fb621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 1.00\n",
            "Decision Tree Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "Approach: Train RandomForestClassifier and use confusion_matrix and seaborn for visualization."
      ],
      "metadata": {
        "id": "RFlXh5_mAELz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute confusion matrix\n",
        "y_pred = rf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "hE_5YARlAEhM",
        "outputId": "daca687c-a790-4373-b729-9ae5008ace70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "Approach: Use StackingClassifier with DecisionTreeClassifier, SVC, and LogisticRegression as base learners, and compare accuracy with individual models."
      ],
      "metadata": {
        "id": "fxPlOYP0AJB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
        "stacking.fit(X_train, y_train)\n",
        "stacking_pred = stacking.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "\n",
        "# Compare with individual models\n",
        "for name, model in estimators:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name.upper()} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkwu1aXdAKBO",
        "outputId": "55e51cdf-3f27-441f-cb05-4a4ad5397f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT Accuracy: 1.00\n",
            "SVM Accuracy: 1.00\n",
            "LR Accuracy: 1.00\n",
            "Stacking Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features.\n",
        "Approach: Train RandomForestClassifier, sort feature importance scores, and print the top 5."
      ],
      "metadata": {
        "id": "iJOUi_A4ANgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_names = data.feature_names\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. Feature: {feature_names[indices[i]]}, Importance: {importances[indices[i]]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYoExo5BANwQ",
        "outputId": "e2f6410b-3ddc-4900-c7fd-cc8f51ac5be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "1. Feature: worst area, Importance: 0.1539\n",
            "2. Feature: worst concave points, Importance: 0.1447\n",
            "3. Feature: mean concave points, Importance: 0.1062\n",
            "4. Feature: worst radius, Importance: 0.0780\n",
            "5. Feature: mean concavity, Importance: 0.0680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "Approach: Train BaggingClassifier and use classification_report for evaluation."
      ],
      "metadata": {
        "id": "6b0PD9WKASZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = bagging.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYSXF7e5ASs-",
        "outputId": "846383ca-3e46-435a-c5de-aa708857c107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.95      0.93      0.94        43\n",
            "      benign       0.96      0.97      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "Approach: Train RandomForestClassifier with varying max_depth and evaluate accuracy."
      ],
      "metadata": {
        "id": "WS-TZnRdAdJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest with different max_depth\n",
        "max_depth_list = [None, 5, 10, 20]\n",
        "for depth in max_depth_list:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Max Depth: {depth}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yBM-SP9AdeJ",
        "outputId": "81378e3c-6260-4fdc-abf0-aabd775ac59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: None, Accuracy: 1.00\n",
            "Max Depth: 5, Accuracy: 1.00\n",
            "Max Depth: 10, Accuracy: 1.00\n",
            "Max Depth: 20, Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "Approach: Train BaggingRegressor with DecisionTreeRegressor and KNeighborsRegressor and compare MSE."
      ],
      "metadata": {
        "id": "mGMHLx6UAlPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Tree\n",
        "bagging_dt = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_pred = bagging_dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Train Bagging Regressor with KNeighbors\n",
        "bagging_knn = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_pred = bagging_knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_pred)\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Bagging with Decision Tree MSE: {dt_mse:.2f}\")\n",
        "print(f\"Bagging with KNeighbors MSE: {knn_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjwx9HiFAlp3",
        "outputId": "bafa56b2-6269-4ad6-a177-153d2fdf03eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Decision Tree MSE: 3222.40\n",
            "Bagging with KNeighbors MSE: 3582.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "Approach: Train RandomForestClassifier and use roc_auc_score for evaluation."
      ],
      "metadata": {
        "id": "VlhGZgDmA6EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate AUC\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uEOTrNZA6hq",
        "outputId": "df9e58c3-6523-4227-b46b-d96b6fbf3782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "Approach: Train BaggingClassifier and use cross_val_score for cross-validation."
      ],
      "metadata": {
        "id": "ELlrS3xWA-Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Bagging Classifier with cross-validation\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "cv_scores = cross_val_score(bagging, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean CV Accuracy: {cv_scores.mean():.2f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IHzeb3lA-2C",
        "outputId": "45f9e775-571b-44f9-ea42-05028240ff0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
            "Mean CV Accuracy: 0.95\n",
            "Standard Deviation: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "Approach: Train RandomForestClassifier and use precision_recall_curve and matplotlib for visualization."
      ],
      "metadata": {
        "id": "JwDfmJEXBDAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and compute precision-recall curve\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "S2aZP1yJBDYO",
        "outputId": "52145714-00a3-496e-c963-575fc73ecca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "Approach: Use StackingClassifier with RandomForestClassifier and LogisticRegression as base learners, and compare accuracy."
      ],
      "metadata": {
        "id": "jfXVe_baBH4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base learners\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
        "stacking.fit(X_train, y_train)\n",
        "stacking_pred = stacking.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "\n",
        "# Compare with individual models\n",
        "for name, model in estimators:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name.upper()} Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "391_vWUoBIMU",
        "outputId": "28bc62d7-a1e9-4a7d-9d14-5af6bee2b01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Accuracy: 1.00\n",
            "LR Accuracy: 1.00\n",
            "Stacking Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 45: Train a Bagging Regressor with different levels of bootstrap samples and compare performance.**\n",
        "\n",
        "\n",
        "Conceptual Approach\n",
        "Objective: The goal is to train a BaggingRegressor from sklearn.ensemble and vary the number of bootstrap samples used to train the base estimators (e.g., Decision Trees). The performance of the model is then compared using a regression metric such as Mean Squared Error (MSE) or RÂ² score.\n",
        "Bootstrap Samples: In the context of BaggingRegressor, the \"level of bootstrap samples\" refers to the size of the data subset used to train each base estimator, controlled by the max_samples parameter. This parameter can be:\n",
        "A float between 0 and 1, representing the fraction of the training data to sample (e.g., 0.5 means 50% of the data is sampled with replacement for each estimator).\n",
        "An integer, representing the exact number of samples to draw (not used here, as fractions are more common for comparison)."
      ],
      "metadata": {
        "id": "_WigNSDDBkBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different levels of bootstrap samples (as fractions of the training set)\n",
        "max_samples_list = [0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "# Train and evaluate Bagging Regressor for each level of bootstrap samples\n",
        "print(\"Performance Comparison for Different Levels of Bootstrap Samples:\")\n",
        "for max_samples in max_samples_list:\n",
        "    # Train Bagging Regressor\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=10,  # Number of base estimators (fixed for comparison)\n",
        "        max_samples=max_samples,  # Fraction of samples used for each estimator\n",
        "        bootstrap=True,  # Enable bootstrap sampling\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = bagging.predict(X_test)\n",
        "\n",
        "    # Evaluate performance\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nMax Samples: {max_samples}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"RÂ² Score: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ0Lt8QzBmnD",
        "outputId": "689597fb-3654-4e44-b16f-f77ce7e3f1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Comparison for Different Levels of Bootstrap Samples:\n",
            "\n",
            "Max Samples: 0.25\n",
            "Mean Squared Error (MSE): 4075.47\n",
            "RÂ² Score: 0.76\n",
            "\n",
            "Max Samples: 0.5\n",
            "Mean Squared Error (MSE): 3219.56\n",
            "RÂ² Score: 0.81\n",
            "\n",
            "Max Samples: 0.75\n",
            "Mean Squared Error (MSE): 3329.64\n",
            "RÂ² Score: 0.80\n",
            "\n",
            "Max Samples: 1.0\n",
            "Mean Squared Error (MSE): 3222.40\n",
            "RÂ² Score: 0.81\n"
          ]
        }
      ]
    }
  ]
}